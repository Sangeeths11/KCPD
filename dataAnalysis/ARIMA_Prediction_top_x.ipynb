{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ARIMA based on top x crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X_CRIMES = 1\n",
    "SAVE_TO_CSV = False\n",
    "ROUND_PREDICTIONS = False\n",
    "SAVE_MODEL_TO_DISK = False\n",
    "SAVE_DICT_TO_DISK = False\n",
    "CURRENT_DATE = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
    "print(\"CURRENT_DATE\", CURRENT_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from math import sqrt\n",
    "from pmdarima import auto_arima\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '..\\\\data\\\\mergedData\\\\ts_df.csv'\n",
    "ts_df = pd.read_csv(path)\n",
    "ts_df = ts_df.loc[:, ~ts_df.columns.str.contains('^Unnamed')]\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series per Crime per District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_ids = ts_df.dist_id.unique()\n",
    "dist_ids = sorted(dist_ids)\n",
    "print(\"dist_ids\", dist_ids)\n",
    "num_dists = len(dist_ids)\n",
    "print(\"num_dists\", num_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = ts_df.Cluster_Name.unique()\n",
    "cluster_names = sorted(cluster_names)\n",
    "print(\"cluster_names\", cluster_names)\n",
    "num_cluster_names = len(cluster_names)\n",
    "print(\"num_cluster_names\", num_cluster_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_dist_dfs = {}\n",
    "for i, dist_id in enumerate(dist_ids):\n",
    "    # Filter data for this district\n",
    "    dist_data = ts_df[ts_df['dist_id'] == dist_id]\n",
    "    crime_dist_dfs[dist_id] = {}\n",
    "    for i, cluster_name in enumerate(cluster_names):\n",
    "        crime_data = dist_data[dist_data['Cluster_Name'] == cluster_name]\n",
    "        # Prepare ts data and split\n",
    "        train = crime_data.loc[crime_data.Reported_Date < \"2024-01-01\"]\n",
    "        test = crime_data.loc[crime_data.Reported_Date >= \"2024-01-01\"]\n",
    "        train = train.groupby(\"Reported_Date\").size().reset_index(name='Crime_Count')\n",
    "        test = test.groupby(\"Reported_Date\").size().reset_index(name='Crime_Count')\n",
    "        train.index = pd.to_datetime(train.Reported_Date)\n",
    "        test.index = pd.to_datetime(test.Reported_Date)\n",
    "        train = train.asfreq('d', fill_value=0)\n",
    "        test = test.asfreq('d', fill_value=0)\n",
    "        train = train.drop(columns=['Reported_Date'])\n",
    "        test = test.drop(columns=['Reported_Date'])\n",
    "        # Save final dict\n",
    "        crime_dist_dfs[dist_id][cluster_name] = {\n",
    "            \"train\": train,\n",
    "            \"test\": test\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of datasets in dict: {num_dists*num_cluster_names}\")\n",
    "print(f\"districts in dict: {crime_dist_dfs.keys()}\")\n",
    "print(f\"crimes in dict: {crime_dist_dfs[1.0].keys()}\")\n",
    "print(f\"test/train in dict: {crime_dist_dfs[1.0]['Alcohol Influence'].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_dist_dfs[1.0]['Alcohol Influence'][\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out crimes with low counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> keep only top x crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_x_crimes(crime_dist_dfs, X = 10):\n",
    "    # Step 1: Aggregate crime counts across dates and districts\n",
    "    total_crime_counts = defaultdict(int)\n",
    "\n",
    "    for district, crimes in crime_dist_dfs.items():\n",
    "        for crime, data_dict in crimes.items():\n",
    "            # Sum the Crime_Count column for the 'train' DataFrame of each crime\n",
    "            total_crime_counts[crime] += data_dict[\"train\"][\"Crime_Count\"].sum()\n",
    "\n",
    "    # Step 2: Sort crimes by count in descending order and extract the top X crimes\n",
    "    X = X  # Specify the number of top crimes to retrieve\n",
    "    top_crimes_set = sorted(total_crime_counts.items(), key=lambda x: x[1], reverse=True)[:X]\n",
    "    \n",
    "    top_crimes = []\n",
    "    # Display the top X crimes\n",
    "    for crime, count in top_crimes_set:\n",
    "        print(f\"{crime}: {count}\")\n",
    "        top_crimes.append(crime)\n",
    "\n",
    "    # Step 3: Filter `crime_dist_dfs` to retain only the top X crimes\n",
    "    filtered_crime_dist_dfs = {\n",
    "        district: {crime: data_dict for crime, data_dict in crimes.items() if crime in top_crimes}\n",
    "        for district, crimes in crime_dist_dfs.items()\n",
    "    }\n",
    "\n",
    "    # `filtered_crime_dist_dfs` now contains only the top X crimes\n",
    "    return filtered_crime_dist_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_dist_dfs = get_top_x_crimes(crime_dist_dfs, X = TOP_X_CRIMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TO_CSV:\n",
    "    # Base path for saving the data\n",
    "    base_path = '..\\\\data\\\\top_10_crimes\\\\'\n",
    "\n",
    "    # Iterate over district IDs, crime types, and data type (train/test)\n",
    "    for district_id, crimes in crime_dist_dfs.items():\n",
    "        for crime_name, data in crimes.items():\n",
    "            \n",
    "            # Create the folder path for each district and crime type\n",
    "            crime_path = os.path.join(base_path, str(district_id), crime_name)\n",
    "            os.makedirs(crime_path, exist_ok=True)  # Create the directories if they don't exist\n",
    "            \n",
    "            # Save the train and test dataframes as CSVs\n",
    "            for data_type, df in data.items():\n",
    "                file_path = os.path.join(crime_path, f\"{data_type}.csv\")\n",
    "                df.to_csv(file_path)\n",
    "        print(f\"saved all files for district {district_id} to disk..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adf_stationary(data):\n",
    "    result = adfuller(data)\n",
    "\n",
    "    if result[1] > 0.05:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL_TO_DISK:\n",
    "    os.makedirs(\"Arima_models\", exist_ok=True)\n",
    "    print(\"Created Arima_models directory..\")\n",
    "\n",
    "# Loop through each district and crime category\n",
    "for dist_id, crimes in crime_dist_dfs.items():\n",
    "    for cluster_name, data_splits in tqdm.tqdm(crimes.items(), desc=f\"dist {dist_id} Crimes\", leave=False):\n",
    "        train_data = data_splits[\"train\"]\n",
    "        test_data = data_splits[\"test\"]\n",
    "        \n",
    "        try:\n",
    "            # Check if stationary\n",
    "            is_stationary = test_adf_stationary(train_data[\"Crime_Count\"])\n",
    "            \n",
    "            # Fit the ARIMA model\n",
    "            #arima_model = auto_arima(train_data, stationary=is_stationary, D=1, start_p=10, max_p=15, start_q=0, max_q=2, seasonal=False, stepwise=True, trace=False, error_action=\"warn\", n_fits=20)\n",
    "            arima_model = auto_arima(train_data, seasonal=False, stationary=is_stationary, stepwise=True, trace=False, error_action=\"ignore\")\n",
    "\n",
    "            # Save model to disk\n",
    "            if SAVE_MODEL_TO_DISK:\n",
    "                model_store_path = os.path.join(\"Arima_models\", str(dist_id), cluster_name)\n",
    "                os.makedirs(model_store_path, exist_ok=True)\n",
    "                print(f\"Created: {model_store_path} directory..\")\n",
    "                # Serialize with Pickle\n",
    "                with open(os.path.join(model_store_path, 'arima.pkl'), 'wb') as pkl:\n",
    "                    pickle.dump(arima_model, pkl)\n",
    "\n",
    "            # Save model configuration\n",
    "            crime_dist_dfs[dist_id][cluster_name][\"model_config\"] = {\n",
    "                \"p\": arima_model.order[0],\n",
    "                \"d\": arima_model.order[1],\n",
    "                \"q\": arima_model.order[2],\n",
    "                \"aic\": arima_model.aic()\n",
    "            }\n",
    "\n",
    "            residuals = arima_model.resid()\n",
    "\n",
    "            # Perform Shapiro-Wilk test on residuals\n",
    "            shapiro_test_statistic, p_value = shapiro(residuals)\n",
    "\n",
    "            # Interpretation\n",
    "            if p_value > 0.05:\n",
    "                raise Exception(\"**Residuals are normally distributed (fail to reject H0).**\")\n",
    "\n",
    "            # Determine the number of predictions\n",
    "            n_predictions = len(test_data) if len(test_data) > 0 else 265\n",
    "            \n",
    "            # Generate predictions\n",
    "            predictions = arima_model.predict(n_periods=n_predictions)\n",
    "\n",
    "            if ROUND_PREDICTIONS:\n",
    "                # Round predictions to the nearest whole numbers\n",
    "                predictions = predictions.round()\n",
    "            \n",
    "            # Calculate RMSE if test data is available\n",
    "            if len(test_data) > 0:\n",
    "                rmse = root_mean_squared_error(test_data, predictions)\n",
    "                crime_dist_dfs[dist_id][cluster_name][\"model_config\"][\"rmse\"] = rmse\n",
    "            else:\n",
    "                crime_dist_dfs[dist_id][cluster_name][\"model_config\"][\"rmse\"] = None\n",
    "\n",
    "            # Save predictions and residuals in data structure\n",
    "            crime_dist_dfs[dist_id][cluster_name][\"forecast\"] = predictions\n",
    "            crime_dist_dfs[dist_id][cluster_name][\"residuals\"] = residuals\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ARIMA model failed for district {dist_id}, crime {cluster_name} with error: {e}\")\n",
    "            crime_dist_dfs[dist_id][cluster_name][\"model_config\"] = None  # If fitting fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crime_forecast(train_data, test_data, forecast_data, crime_type=\"Unknown Crime\", district_id=\"Unknown District\", plot_train=True):\n",
    "    \"\"\"\n",
    "    Plots the training data, test data, and forecasted data in one plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    if plot_train:\n",
    "        # Plot training data\n",
    "        plt.plot(train_data, label=\"Training Data\", color=\"blue\")\n",
    "    \n",
    "    # Plot test data\n",
    "    plt.plot(test_data, label=\"Test Data (Actual)\", color=\"green\")\n",
    "    \n",
    "    # Plot forecast data\n",
    "    plt.plot(forecast_data, label=\"Predictions\", color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # Formatting the plot\n",
    "    plt.title(f\"Crime Forecast for {crime_type} in District {district_id}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Crimes\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district = 1.0\n",
    "crime_name = 'Auto Theft'\n",
    "plot_crime_forecast(crime_dist_dfs[district][crime_name][\"train\"], crime_dist_dfs[district][crime_name][\"test\"], crime_dist_dfs[district][crime_name][\"forecast\"], crime_type=crime_name, district_id=district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district = 1.0\n",
    "crime_name = 'Auto Theft'\n",
    "plot_crime_forecast(crime_dist_dfs[district][crime_name][\"train\"], crime_dist_dfs[district][crime_name][\"test\"], crime_dist_dfs[district][crime_name][\"forecast\"], crime_type=crime_name, district_id=district, plot_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aic_values(crime_dist_dfs):\n",
    "    # Step 1: Get all districts and crimes\n",
    "    districts = list(crime_dist_dfs.keys())\n",
    "    num_districts = len(districts)\n",
    "    crimes = {crime for district in crime_dist_dfs.values() for crime in district.keys()}\n",
    "    num_crimes = len(crimes)\n",
    "\n",
    "    # Step 2: Set up subplots based on number of districts and crimes\n",
    "    fig, axs = plt.subplots(num_districts, 1, figsize=(10, num_districts * 5))\n",
    "    fig.suptitle('AIC values for Different Crimes across Districts')\n",
    "\n",
    "    # Step 3: Populate each subplot\n",
    "    for idx, district in enumerate(districts):\n",
    "        aic_values = []\n",
    "        crime_names = []\n",
    "        \n",
    "        for crime in crime_dist_dfs[district].keys():\n",
    "            aic = crime_dist_dfs[district][crime][\"model_config\"][\"aic\"]\n",
    "            aic_values.append(aic)\n",
    "            crime_names.append(crime)\n",
    "        \n",
    "        ax = axs[idx] if num_districts > 1 else axs  # Adjust if single subplot\n",
    "        ax.bar(crime_names, aic_values)\n",
    "        ax.set_title(f'AIC values in {district}')\n",
    "        ax.set_ylabel('AIC')\n",
    "        ax.set_xlabel('Crime Type')\n",
    "        ax.set_xticks(crime_names)\n",
    "        ax.set_xticklabels(crime_names, rotation=25)  # Rotate x-tick labels\n",
    "        for i, v in enumerate(aic_values):\n",
    "            ax.text(i, v, f\"{v:.2f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aic_values(crime_dist_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_heatmap(crime_dist_dfs):\n",
    "    \"\"\"\n",
    "    Function to create a heatmap plot of RMSE values for crimes across districts\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold RMSE values\n",
    "    rmse_data = {\n",
    "        \"District\": [],\n",
    "        \"Crime\": [],\n",
    "        \"RMSE\": []\n",
    "    }\n",
    "    \n",
    "    # Collect RMSE values for each crime in each district\n",
    "    for dist_id, crimes in crime_dist_dfs.items():\n",
    "        for crime_name, crime_info in crimes.items():\n",
    "            if crime_info.get(\"model_config\") and crime_info[\"model_config\"].get(\"rmse\") is not None:\n",
    "                rmse_data[\"District\"].append(dist_id)\n",
    "                rmse_data[\"Crime\"].append(crime_name)\n",
    "                rmse_data[\"RMSE\"].append(crime_info[\"model_config\"][\"rmse\"])\n",
    "    \n",
    "    # Create a DataFrame from the collected RMSE data\n",
    "    rmse_df = pd.DataFrame(rmse_data)\n",
    "    rmse_pivot = rmse_df.pivot(index=\"Crime\", columns=\"District\", values=\"RMSE\")\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(rmse_pivot, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'RMSE'})\n",
    "    plt.title(\"RMSE of Crime Predictions by District and Crime Category\")\n",
    "    plt.xlabel(\"District\")\n",
    "    plt.ylabel(\"Crime Category\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rmse_heatmap(crime_dist_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DICT_TO_DISK:\n",
    "    # Save dictionary to a pickle file\n",
    "    with open(f\"..\\\\data\\\\mergedData\\\\crime_dist_dfs_top_{TOP_X_CRIMES}_{CURRENT_DATE}.pkl\", 'wb') as file:\n",
    "        pickle.dump(crime_dist_dfs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary from a pickle file\n",
    "#with open(f\"..\\\\data\\\\mergedData\\\\crime_dist_dfs_top_{TOP_X_CRIMES}.pkl\", 'rb') as file:\n",
    "#    crime_dist_dfs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_cds122",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
